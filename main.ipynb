{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo 3\n",
    "Alumnos:\n",
    "- Marcos Hernández Rodríguez\n",
    "- Salva ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Sobre la base de datos de peatones para clasificación. Aplicar un clasificador SVM. Se deberá implementar HoG (no usar una función ya implementada). Obtener resultados de bondad del clasificador\n",
    "2. Sobre la base de datos de peatones para clasificación. Aplicar un clasificador usando una red neuronal de convolución. Se deberán reusar y modificar las arquitecturas VGG16, ResNet18 y Xception para tal fin. Obtener una comparativa de bondad usando predictores como agudeza, F1, recall, precision.\n",
    "3. Sobre la base de datos de peatones para clasificación. Compara la mejor red neuronal obtenida en el punto 2 con el clasificador SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "924\n"
     ]
    }
   ],
   "source": [
    "# Preparar los datos para las imágenes con personas andando\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "pedestrians_path = 'input/classification/pedestrians128x64/'\n",
    "pedestrians_images = load_images_from_folder(pedestrians_path)\n",
    "print(type(pedestrians_images))\n",
    "print(len(pedestrians_images))\n",
    "print(pedestrians_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "400\n",
      "(128, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# Select 400 images randomly\n",
    "import random\n",
    "pedestrians_images = random.sample(pedestrians_images, 400)\n",
    "print(type(pedestrians_images))\n",
    "print(len(pedestrians_images))\n",
    "print(pedestrians_images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "50\n",
      "(256, 256, 3)\n",
      "<class 'list'>\n",
      "40\n",
      "(256, 256, 3)\n",
      "(128, 64, 3)\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# Preparar los datos para las imágenes sin personas andando\n",
    "no_pedestrians_path = 'input/classification/pedestrians_neg/'\n",
    "no_pedestrians_images = load_images_from_folder(no_pedestrians_path)\n",
    "print(type(no_pedestrians_images))\n",
    "print(len(no_pedestrians_images))\n",
    "print(no_pedestrians_images[0].shape)\n",
    "\n",
    "# Select images randomly\n",
    "import random\n",
    "no_pedestrians_images = random.sample(no_pedestrians_images, 40)\n",
    "print(type(no_pedestrians_images))\n",
    "print(len(no_pedestrians_images))\n",
    "print(no_pedestrians_images[0].shape)\n",
    "\n",
    "# Resize de las imagenes a 512x512\n",
    "import cv2\n",
    "for i, img in enumerate(no_pedestrians_images):\n",
    "    no_pedestrians_images[i] = cv2.resize(no_pedestrians_images[i], (512, 512))\n",
    "\n",
    "    # Escoger un trozo de 128x64\n",
    "    no_pedestrians_images[i] = no_pedestrians_images[i][192:320, 224:288]\n",
    "\n",
    "print(no_pedestrians_images[0].shape)\n",
    "print(len(no_pedestrians_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caracterizar la imagen usando Histograma de Orientación de Gradiente (HOG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18768\\222623496.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mped_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "ped_img.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18768\\1191862949.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_gradient_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpedestrians_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\marco\\Google Drive\\02. Máster\\12. VisiónComp\\Trabajo 3 - Clasificacion y Segmentacion de Peatones (Teoría 4 Puntos)\\modules\\helpers.py\u001b[0m in \u001b[0;36mget_gradient_matrix\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_gradient_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "print(h.get_gradient_matrix(pedestrians_images[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "128\n",
      "<class 'numpy.ndarray'>\n",
      "(128, 64, 3)\n",
      "<class 'int'> 128\n",
      "<class 'int'> 64\n",
      "gradient_matrix shape: (128, 64)\n",
      "--- split_in_cells ---\n",
      "gradient_matrix shape: (128, 64)\n",
      "cell_rows: 16\n",
      "cell_columns: 8\n",
      "(8, 8)\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.           0.           0.        ]\n",
      " [  0.         210.0101188   60.61745504  65.64914972  44.27753381\n",
      "   36.97018866  14.07351181          nan]\n",
      " [  0.         183.129203   145.4547377   82.81660185  75.07481538\n",
      "    0.                  nan   7.4915149 ]\n",
      " [  0.          72.45881961 145.99290123 235.94885364 196.33700234\n",
      "  188.96447813 243.35265698          nan]\n",
      " [  0.                  nan  18.33471059   0.         130.10598891\n",
      "           nan 185.67603383  35.17703516]\n",
      " [  0.         158.47912777 179.06950162   3.99655708   0.\n",
      "   25.9569329   22.96637544 144.27681633]\n",
      " [  0.          66.64457967 173.20914274          nan   2.25258427\n",
      "  171.71088723  23.96944752 176.54231507]\n",
      " [  0.          82.82173674 177.02358147   0.           0.\n",
      "  121.96888009  56.82227741 180.4129055 ]]\n",
      "()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\Google Drive\\02. Máster\\12. VisiónComp\\Trabajo 3 - Clasificacion y Segmentacion de Peatones (Teoría 4 Puntos)\\modules\\helpers.py:38: RuntimeWarning: invalid value encountered in sqrt\n",
      "  norm = np.sqrt(0.5 * (brackets))\n",
      "c:\\Users\\marco\\Google Drive\\02. Máster\\12. VisiónComp\\Trabajo 3 - Clasificacion y Segmentacion de Peatones (Teoría 4 Puntos)\\modules\\helpers.py:36: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  argument = 0.5 * np.arctan(2*gXY / (gXX-gYY))\n",
      "c:\\Users\\marco\\Google Drive\\02. Máster\\12. VisiónComp\\Trabajo 3 - Clasificacion y Segmentacion de Peatones (Teoría 4 Puntos)\\modules\\helpers.py:36: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  argument = 0.5 * np.arctan(2*gXY / (gXX-gYY))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18768\\2782607431.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m### SEGUNDA FASE: Obtener la división en células\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_in_cells\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marco\\Google Drive\\02. Máster\\12. VisiónComp\\Trabajo 3 - Clasificacion y Segmentacion de Peatones (Teoría 4 Puntos)\\modules\\helpers.py\u001b[0m in \u001b[0;36msplit_in_cells\u001b[1;34m(gradient_matrix, cell_size)\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell_img\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from modules import helpers as h\n",
    "\n",
    "for i, ped_img in enumerate(pedestrians_images):\n",
    "    print(i)\n",
    "\n",
    "    ### PRIMERA FASE: Obtener el gradiente de cada píxel\n",
    "    gradient_matrix = h.get_gradient_matrix(ped_img)\n",
    "    print(f'gradient_matrix shape: {gradient_matrix.shape}')\n",
    "\n",
    "    ### SEGUNDA FASE: Obtener la división en células\n",
    "    print(h.split_in_cells(gradient_matrix, cell_size=8))\n",
    "\n",
    "    if i == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 4. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "a = np.zeros((4, 4))\n",
    "print(a)\n",
    "a[1, 1] = 4\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HOG_X_pos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18768\\251879158.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mHOG_X_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mHOG_X_neg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#Concatenación de HOG\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_pos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_neg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m#Concatenación de valor de la clase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mntrain\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;31m#cardinal del conjunto entrenamiento\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mntest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mntrain\u001b[0m\u001b[1;33m;\u001b[0m \u001b[1;31m#cardinal del conjunto test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'HOG_X_pos' is not defined"
     ]
    }
   ],
   "source": [
    "X=np.concatenate([HOG_X_pos,HOG_X_neg])#Concatenación de HOG\n",
    "y=np.concatenate([y_pos,y_neg]); #Concatenación de valor de la clase\n",
    "ntrain= round(0.8*len(y));#cardinal del conjunto entrenamiento\n",
    "ntest = len(x)-ntrain; #cardinal del conjunto test\n",
    "\n",
    "idx = np.random.permutation(np.arange(len(y))) #obtenemos una permutación\n",
    "X_train= X[idx[0:ntrain],:]; #Obtenemos barajado el conjunto entrenamiento\n",
    "y_train=y[idx[0:ntrain]] #igual para el valor de clase\n",
    "X_test= X[idx[ntrain+1:len(y)],:];#Obtenemos barajado el conjunto test\n",
    "y_test=y[idx[ntrain+1:len(y)]]; #igual para el valor de clase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "m=SVC();\n",
    "m.fit(X_train,y_train)\n",
    "\n",
    "y_pred_train=m.predict(X_train); \n",
    "\n",
    "y_pred_test=m.predict(X_test); \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train,y_pred_train)\n",
    "confusion_matrix(y_test,y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with CNN\n",
    "\n",
    "2. Sobre la base de datos de peatones para clasificación. Aplicar un clasificador usando una red neuronal de convolución. Se deberán reusar y modificar las arquitecturas VGG16, ResNet18 y Xception para tal fin. Obtener una comparativa de bondad usando predictores como agudeza, F1, recall, precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16, ResNet18 y Xception.\n",
    "# Dijo Sara que se puede usar ResNet50 en vez de la 18 si no funciona bien la 18\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sobre la base de datos de peatones para clasificación. Compara la mejor red neuronal obtenida en el punto 2 con el clasificador SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation\n",
    "\n",
    "Sobre la base de datos de peatones para segmentación semántica. Partiendo de la\n",
    "arquitectura base probar diferentes modificaciones de esta. Realizar un análisis de las\n",
    "arquitecturas estudiadas:\n",
    "- Obtener en este análisis el promedio de IoU (intersection over union) a lo largo del conjunto test sobre las diferentes arquitecturas estudiadas. Obtener la agudeza.\n",
    "- Tiempo de aprendizaje.\n",
    "- Dimensión de la representación latente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El alumno/a debe tener en cuenta que las imágenes tienen diferentes dimensiones tanto en el conjunto de entrenamiento como en el conjunto test. Por lo tanto un paso previo será normalizar las imágenes a la misma dimensión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "96\n",
      "(406, 612, 3)\n",
      "(378, 745, 3)\n",
      "(418, 670, 3)\n",
      "(436, 786, 3)\n",
      "(454, 767, 3)\n",
      "<class 'list'>\n",
      "96\n",
      "(406, 612, 3)\n",
      "(378, 745, 3)\n",
      "(418, 670, 3)\n",
      "(436, 786, 3)\n",
      "(454, 767, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read data training images\n",
    "data_segmentation_train_imgs_path = 'input/data_segmentation/train/images/'\n",
    "data_seg_train_imgs = load_images_from_folder(data_segmentation_train_imgs_path)\n",
    "print(type(data_seg_train_imgs))\n",
    "print(len(data_seg_train_imgs))\n",
    "for i in [0, 1, 2, 3, 4]:\n",
    "    print(data_seg_train_imgs[i].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read data training masks\n",
    "data_segmentation_train_masks_path = 'input/data_segmentation/train/mask/'\n",
    "data_seg_train_masks = load_images_from_folder(data_segmentation_train_masks_path)\n",
    "print(type(data_seg_train_masks))\n",
    "print(len(data_seg_train_masks))\n",
    "for i in [0, 1, 2, 3, 4]:\n",
    "    print(data_seg_train_masks[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "74\n",
      "(536, 559, 3)\n",
      "(414, 455, 3)\n",
      "(445, 479, 3)\n",
      "(397, 396, 3)\n",
      "(344, 335, 3)\n",
      "<class 'list'>\n",
      "74\n",
      "(536, 559, 3)\n",
      "(414, 455, 3)\n",
      "(445, 479, 3)\n",
      "(397, 396, 3)\n",
      "(344, 335, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read data test images\n",
    "data_segmentation_test_imgs_path = 'input/data_segmentation/test/images/'\n",
    "data_seg_test_imgs = load_images_from_folder(data_segmentation_test_imgs_path)\n",
    "print(type(data_seg_test_imgs))\n",
    "print(len(data_seg_test_imgs))\n",
    "for i in [0, 1, 2, 3, 4]:\n",
    "    print(data_seg_test_imgs[i].shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read data test masks\n",
    "data_segmentation_test_masks_path = 'input/data_segmentation/test/mask/'\n",
    "data_seg_test_masks = load_images_from_folder(data_segmentation_test_masks_path)\n",
    "print(type(data_seg_test_masks))\n",
    "print(len(data_seg_test_masks))\n",
    "for i in [0, 1, 2, 3, 4]:\n",
    "    print(data_seg_test_masks[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "96\n",
      "74\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "print(len(data_seg_train_imgs))\n",
    "print(len(data_seg_train_masks))\n",
    "\n",
    "print(len(data_seg_test_imgs))\n",
    "print(len(data_seg_test_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv_1\" is incompatible with the layer: expected min_ndim=5, found ndim=4. Full shape received: (None, 214, 214, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18768\\518100563.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mInputLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m214\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m214\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'imageinput'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mConv3D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"same\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'conv_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# salida de capa de 214x214x64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu_1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 629\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    630\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\marco\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[0;32m    229\u001b[0m                          \u001b[1;34m'is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                          \u001b[1;34mf'expected min_ndim={spec.min_ndim}, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"conv_1\" is incompatible with the layer: expected min_ndim=5, found ndim=4. Full shape received: (None, 214, 214, 3)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import InputLayer, ReLU, Conv3D, Conv3DTranspose, Softmax, Dense\n",
    "\n",
    "# https://keras.io/api/layers/\n",
    "\n",
    "# Creamos el modelo\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "# Falla algo de las dimensiones\n",
    "model.add(InputLayer(input_shape=(214, 214, 3), name='imageinput'))\n",
    "model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding=\"same\", name='conv_1'))\n",
    "# salida de capa de 214x214x64\n",
    "model.add(ReLU(name='relu_1'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding=(0,0,0,0), name='maxpool_1'))\n",
    "# salida de capa de 107x107x64\n",
    "model.add(tf.keras.layers.Conv3D(64, (3,3,64), strides=(1,1), padding=\"same\", name='conv_2'))\n",
    "model.add(ReLU(name='relu_2'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding=(0,0,0,0), name='maxpool_2'))\n",
    "model.add(tf.keras.layers.Conv3D(64, (3,3,64), strides=(1,1), padding=\"same\", name='conv_3'))\n",
    "model.add(ReLU(name='relu_3'))\n",
    "model.add(tf.keras.layers.Conv3D(64, (3,3,64), strides=(1,1), padding=\"same\", name='conv_4'))\n",
    "\n",
    "# Capas decodificadoras\n",
    "# cropping es lo mismo que padding?\n",
    "model.add(Conv3DTranspose(64, (4,4,64), strides=(2, 2), padding=(0,0,0,0), name='transposed-conv_1'))\n",
    "model.add(Conv3DTranspose(64, (4,4,64), strides=(2, 2), padding=(0,0,0,0), name='transposed-conv_2'))\n",
    "\n",
    "model.add(tf.keras.layers.Conv3D(2, (1,1,64), strides=(1,1), padding=(0,0,0,0), name='conv_5'))\n",
    "# A continuación aplicamos una capa softmax con función de perdida entropia cruzada para obtener la clases fondo o peatón.\n",
    "model.add(Softmax(name='softmax'))\n",
    "# ? es una dense?\n",
    "model.add(Dense(1, activation='sigmoid', name='classoutput'))\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"BinaryCrossentropy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5e1398cdbc5000f70d27840b9ea3342abf7ef0093b1dcda8062c3b12392418e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
